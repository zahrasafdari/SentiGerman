{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, RobertaTokenizer, RobertaModel\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a custom dataset\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, max_len):\n",
        "        self.data = pd.read_csv(file_path)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.data.iloc[index, 0])\n",
        "        label = self.data.iloc[index, 1]\n",
        "        if label == 'positive':\n",
        "            label = 1\n",
        "        elif label == 'negative':\n",
        "            label = 2\n",
        "        else:\n",
        "            label = 0\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Define custom models\n",
        "class BERTModel(nn.Module):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super(BERTModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "        self.fc = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.pooler_output\n",
        "        logits = self.fc(sequence_output)\n",
        "        return logits\n",
        "\n",
        "class RoBERTaModel(nn.Module):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super(RoBERTaModel, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(model_name)\n",
        "        self.fc = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.pooler_output\n",
        "        logits = self.fc(sequence_output)\n",
        "        return logits\n",
        "\n",
        "# Load data\n",
        "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-german-dbmdz-uncased')\n",
        "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "train_dataset = SentimentDataset('.../sentiment-train.csv', tokenizer_bert, max_len=128)\n",
        "test_dataset = SentimentDataset('..../sentiment-test.csv', tokenizer_bert, max_len=128)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize models\n",
        "model_bert = BERTModel('bert-base-german-dbmdz-uncased', num_labels=3)\n",
        "model_roberta = RoBERTaModel('roberta-base', num_labels=3)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_bert.to(device)\n",
        "model_roberta.to(device)\n",
        "\n",
        "# Training function\n",
        "def train(models, train_loader, test_loader, epochs=3):\n",
        "    optimizers = [AdamW(model.parameters(), lr=2e-5) for model in models]\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    schedulers = [get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps) for optimizer in optimizers]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for model in models:\n",
        "            model.train()\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            logits_list = []\n",
        "            loss_list = []\n",
        "\n",
        "            for model, optimizer in zip(models, optimizers):\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                logits_list.append(logits)\n",
        "                loss_fct = nn.CrossEntropyLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "                loss_list.append(loss)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            for scheduler in schedulers:\n",
        "                scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} completed.\")\n",
        "        evaluate(models, test_loader)\n",
        "\n",
        "def evaluate(models, test_loader):\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            logits_list = []\n",
        "            for model in models:\n",
        "                logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                logits_list.append(logits)\n",
        "\n",
        "            # Average predictions from all models\n",
        "            avg_logits = torch.mean(torch.stack(logits_list), dim=0)\n",
        "            _, predicted = torch.max(avg_logits, dim=1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Calculate micro F1 score\n",
        "    f1 = f1_score(all_labels, all_predictions, average='micro')\n",
        "    print(f\"Micro F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Train the models\n",
        "train([model_bert, model_roberta], train_loader, test_loader, epochs=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZygq6PaoizJ",
        "outputId": "a844ea64-38fa-4744-c3ec-1ecdda022768"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 completed.\n",
            "Micro F1 Score: 0.7761\n",
            "Epoch 2/3 completed.\n",
            "Micro F1 Score: 0.7941\n",
            "Epoch 3/3 completed.\n",
            "Micro F1 Score: 0.7914\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
